{"cells":[{"cell_type":"markdown","id":"9bd2d03e-a35d-42df-ad50-0ebcac7ff18f","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Load Test Results\n","This notebook takes CSV files located in the raw folder, processes the files, and loads the test results in the TestResults table, and updates the ProjectInformation and Calendar tables."]},{"cell_type":"markdown","id":"2c981082-6788-4859-9d5a-f199f826ca7d","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Parameters for this Notebook"]},{"cell_type":"markdown","id":"a39f97e6-c01f-4e68-900f-8ebfd7ce937a","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":[]},{"cell_type":"code","execution_count":19,"id":"4328773a-fcf5-4c63-aef2-3e470cd11e21","metadata":{"editable":true,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"tags":["parameters"]},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:34.9164261Z","execution_start_time":"2024-06-25T13:59:34.6758472Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"9b322c72-3897-43cd-819c-1b80f5e346fd","queued_time":"2024-06-25T13:59:34.3589424Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":21,"statement_ids":[21]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 21, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["workspace_id = \"x3dbac5b-48be-41eb-83a7-6c178940703x\"\n","lakehouse_id = \"x754c80f-5f13-40b1-ab93-e4368da923cx\"\n"]},{"cell_type":"markdown","id":"fc86d6d8-8499-4594-a1df-9e4b29a9def4","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Import Libraries\n","* Imports the pyspark libraries to support data cleansing\n","* Imports the delta tables and set merging capabilities\n","* Imports os for file and folder manipulations\n","* Imports notebook utilities to mount lakehouse for automation purposes"]},{"cell_type":"code","execution_count":20,"id":"12e2ee71-fda3-4528-b927-c8e01a282891","metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:35.7702556Z","execution_start_time":"2024-06-25T13:59:35.5247786Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"23c2ab82-67b2-4361-adab-02340728d95f","queued_time":"2024-06-25T13:59:34.443895Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":22,"statement_ids":[22]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 22, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["# Import Libraries\n","from pyspark.sql.functions import (\n","    col, to_date, to_timestamp, to_utc_timestamp, year, month, date_format, split,\n","    hour, minute, second, expr,\n","    lit, concat, row_number, when\n",")\n","from pyspark.sql.window import Window\n","from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType, IntegerType\n","\n","# For merging tables\n","from delta.tables import *\n","spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n","\n","# For file movement\n","import os\n","\n","# For data calculations\n","from datetime import timedelta\n","from math import ceil\n","\n","# Mount Lakehouse programmatically\n","from notebookutils import mssparkutils"]},{"cell_type":"markdown","id":"80dc9a2d-63de-4eca-8449-e74164a33fbf","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Setup Lakehouse Connections\n","* Setup variables to support storing and processing test results\n","* Mount Lakehouse"]},{"cell_type":"code","execution_count":21,"id":"82eda070-bc52-42b0-ab09-2b5a54e39990","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:37.0451254Z","execution_start_time":"2024-06-25T13:59:36.2310483Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"cef5bec8-d255-4de0-85f7-5589071a336d","queued_time":"2024-06-25T13:59:34.5298465Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":23,"statement_ids":[23]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 23, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.synapse.widget-view+json":{"widget_id":"a388ce29-fdd7-4b9b-bad2-12986ddf75c6","widget_type":"Synapse.DataFrame"},"text/plain":["SynapseWidget(Synapse.DataFrame, a388ce29-fdd7-4b9b-bad2-12986ddf75c6)"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.synapse.widget-view+json":{"widget_id":"94d0bed7-1df3-4976-97ba-100160c618eb","widget_type":"Synapse.DataFrame"},"text/plain":["SynapseWidget(Synapse.DataFrame, 94d0bed7-1df3-4976-97ba-100160c618eb)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["/synfs/notebook/06114e8d-f378-47a7-b2c1-bdd22ea1b5f7/lakehouse/default\n"]}],"source":["# Thanks to Sandeep Pawar\n","# https://fabric.guru/how-to-mount-a-lakehouse-and-identify-the-mounted-lakehouse-in-fabric-notebook\n","\n","# Set Path Information\n","folder_path = \"https://onelake.dfs.fabric.microsoft.com/\" + workspace_id + \"/\" + lakehouse_id + \"/\"\n","abfss_path = \"abfss://\" + workspace_id + \"@onelake.dfs.fabric.microsoft.com/\" + lakehouse_id\n","relative_path = \"Files/DQVTests/raw\"\n","processing_path = relative_path + \"/processing/\"\n","processed_path = relative_path + \"/processed/\"\n","mount_name = \"/lakehouse/default\"\n","\n","# Mount Lakehouse\n","mssparkutils.fs.mount(abfss_path, mount_name)\n","\n","mount_points = mssparkutils.fs.mounts()\n","local_path = next((mp[\"localPath\"] for mp in mount_points if mp[\"mountPoint\"] == mount_name), None)\n","\n","print(local_path)\n","\n","# Retrieve Tables\n","tables = os.listdir(local_path + \"/Tables\")\n"]},{"cell_type":"markdown","id":"449fd381-e50d-4350-9bb8-f28722a7b4cf","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Create Folders\n","* Boostrapping code creates folders if that don't exist already"]},{"cell_type":"code","execution_count":22,"id":"7f5ad6d2-fced-4fb1-b32f-8806adc7ea14","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:37.7231687Z","execution_start_time":"2024-06-25T13:59:37.4895204Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"1b8b2f1c-b7e2-463f-a708-751a1ff9ad50","queued_time":"2024-06-25T13:59:34.6285529Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":24,"statement_ids":[24]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 24, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["# Check if Raw directory exists, if not, create it\n","if not os.path.exists(local_path + \"/\" + relative_path):\n","    os.makedirs(local_path + \"/\" + relative_path)\n","\n","if not os.path.exists(local_path + \"/\" + processed_path):\n","    os.makedirs(local_path + \"/\" + processed_path)\n","\n","if not os.path.exists(local_path + \"/\" + processing_path):\n","    os.makedirs(local_path + \"/\" + processing_path)"]},{"cell_type":"markdown","id":"4bc475a9-c04a-4235-8565-89847fd97664","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Identify Files for processing"]},{"cell_type":"code","execution_count":23,"id":"9722ba2e-b3ef-4ef4-89c5-941769d40196","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:38.9603119Z","execution_start_time":"2024-06-25T13:59:38.1437233Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"76f64961-99ec-4b75-83f6-92090d4c96dd","queued_time":"2024-06-25T13:59:34.7362562Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":25,"statement_ids":[25]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 25, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["['abfss://c3dbac5b-48be-41eb-83a7-6c1789407037@onelake.dfs.fabric.microsoft.com/a754c80f-5f13-40b1-ab93-e4368da923c4/Files/DQVTests/raw/2024-06-25T13-53-03Z-5530fe89-403a-4a59-af46-83357d80839a.csv', 'abfss://c3dbac5b-48be-41eb-83a7-6c1789407037@onelake.dfs.fabric.microsoft.com/a754c80f-5f13-40b1-ab93-e4368da923c4/Files/DQVTests/raw/2024-06-25T13-55-07Z-c7518fe9-184b-4264-bb5d-04cd517ae28a.csv']\n","['abfss://c3dbac5b-48be-41eb-83a7-6c1789407037@onelake.dfs.fabric.microsoft.com/a754c80f-5f13-40b1-ab93-e4368da923c4/Files/DQVTests/raw/processing/2024-06-25T13-53-03Z-5530fe89-403a-4a59-af46-83357d80839a.csv', 'abfss://c3dbac5b-48be-41eb-83a7-6c1789407037@onelake.dfs.fabric.microsoft.com/a754c80f-5f13-40b1-ab93-e4368da923c4/Files/DQVTests/raw/processing/2024-06-25T13-55-07Z-c7518fe9-184b-4264-bb5d-04cd517ae28a.csv']\n"]}],"source":["# Get list of file names from local directory\n","file_names = os.listdir(local_path + \"/\" + relative_path)\n","\n","# Initialize lists for source and target file paths\n","target_file_names = []\n","source_file_names = []\n","\n","# Iterate through file names\n","for i, item in enumerate(file_names):\n","    # Check if file ends with '.csv'\n","    if item.endswith('.csv'):\n","        # Construct source and target paths\n","        temp_source = abfss_path + \"/\" + relative_path + \"/\" + item\n","        temp_target = abfss_path + \"/\" + processing_path + item\n","        \n","        # Add paths to respective lists\n","        source_file_names.append(temp_source)\n","        target_file_names.append(temp_target)\n","        \n","        # Move files from source to target location\n","        mssparkutils.fs.mv(temp_source, temp_target, overwrite=True)\n","        continue\n","\n","# Print lists of source and target file paths\n","print(source_file_names)\n","print(target_file_names)\n"]},{"cell_type":"markdown","id":"c86a7c3b-e6ae-4f61-9b43-5e2e6b5797db","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Add additional columns to test files and set types"]},{"cell_type":"code","execution_count":24,"id":"413874a9-0333-4a1b-a65e-682db23e4432","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:40.2829019Z","execution_start_time":"2024-06-25T13:59:39.4575986Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"84a8dc41-14e4-441e-9c7a-f9f18de1d4f5","queued_time":"2024-06-25T13:59:34.8354854Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":26,"statement_ids":[26]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 26, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["if len(target_file_names) > 0:\n","    # Read Files\n","    df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n","                                 .option(\"quote\", '\"') \\\n","                                 .option(\"escape\", '\"') \\\n","                                 .load(target_file_names)\n","\n","    # Create Concatenated Key to help with the merge statement\n","    df = df.withColumn(\"ConcatenatedKey\", concat(df[\"RunID\"], lit('|'), df[\"Order\"]))\n","\n","    # Extract the date part from the datetime column\n","    df = df.withColumn(\"RunDate\", to_date(col(\"RunDateTime\")))\n","\n","    # Setup project specific information\n","    df = df.withColumn(\"ProjectConcatenatedKey\", concat(col(\"ModelName\"), lit(\"|\"),\n","                                                        col(\"BranchName\"), lit(\"|\"),\n","                                                        col(\"RepositoryName\"), lit(\"|\"),\n","                                                        col(\"ProjectName\")))\n","\n","    # Extract various components from the datetime column\n","    df = df.withColumn(\"RunYear\", year(df['RunDate']))\n","    df = df.withColumn(\"RunMonth\", month(df['RunDate']))\n","    df = df.withColumn(\"RunDay\", date_format(df['RunDate'], \"d\").cast('int'))\n","    df = df.withColumn(\"RunTimeStr\", split(df['RunDateTime'], 'T')[1])\n","    df = df.withColumn(\"RunHour\", split(df['RunTimeStr'], '-')[0])\n","    df = df.withColumn(\"RunMinute\", split(df['RunTimeStr'], '-')[1])\n","    df = df.withColumn(\"RunSeconds\", split(split(df['RunTimeStr'], '-')[2], 'Z')[0])\n","    df = df.withColumn(\"RunTime\", to_timestamp(concat(lit(\"1970-1-1 \"),\n","                                                      col(\"RunHour\"), lit(\":\"), \n","                                                      col(\"RunMinute\"), lit(\":\"), \n","                                                      col(\"RunSeconds\"), lit(\" Z\"))))\n"]},{"cell_type":"markdown","id":"87e10338-00b3-4dd3-9827-3b993527f298","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Upsert to Project Information Table"]},{"cell_type":"code","execution_count":25,"id":"921f4e05-6083-4604-ae43-0f89d4b177fe","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:47.1555768Z","execution_start_time":"2024-06-25T13:59:40.7137337Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"c840674b-fa11-450a-8d5d-35e3f7fc268c","queued_time":"2024-06-25T13:59:34.9502088Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":27,"statement_ids":[27]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 27, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["if len(target_file_names) > 0:\n","    # Define schema for Test Results\n","    schema_proj_info = StructType([\n","        StructField(\"ProjectConcatenatedKey\", StringType(), True),\n","        StructField(\"ProjectName\", StringType(), True),\n","        StructField(\"RepositoryName\", StringType(), True),\n","        StructField(\"BranchName\", StringType(), True),\n","        StructField(\"ArtifactName\", StringType(), True),\n","        StructField(\"ArtifactType\", StringType(), True),\n","        StructField(\"ProjectInformationID\", IntegerType(), True),\n","    ])\n","\n","    # Select relevant columns and filter out rows where 'ProjectConcatenatedKey' is null\n","    prelim_project_df = df.select(\n","        \"ProjectConcatenatedKey\",\n","        \"ProjectName\",\n","        \"RepositoryName\",\n","        \"BranchName\",\n","        \"ModelName\",\n","    ).where(col('ProjectConcatenatedKey').isNotNull())\n","\n","    prelim_project_df = prelim_project_df.withColumnRenamed(\"ModelName\",\"ArtifactName\")\\\n","                                         .withColumn(\"ArtifactType\",lit(\"Semantic Model\"))\n","\n","    # Determine the maximum ProjectInformationID to generate unique identifiers\n","    if not (\"ProjectInformation\" in tables):\n","        max_id = 0\n","    else:\n","        max_project_df = spark.sql(\"SELECT MAX(ProjectInformationID) as max FROM ProjectInformation\")\n","        max_id = max_project_df.first()['max']\n","\n","    # Generate unique identifiers using window function and update prelim_project_df\n","    w = Window().orderBy(lit('ProjectConcatenatedKey'))\n","    prelim_project_df = prelim_project_df.dropDuplicates(['ProjectConcatenatedKey'])\n","    prelim_project_df = prelim_project_df.withColumn(\"ProjectInformationID\", lit(max_id) + row_number().over(w))\n","\n","    # Enforce the schema when creating DataFrame\n","    project_df = spark.createDataFrame(prelim_project_df.toPandas(), schema_proj_info)\n","\n","    # Load or overwrite project information based on table existence\n","    if \"ProjectInformation\" in tables:\n","        project_delta_df = DeltaTable.forPath(spark, \"Tables/ProjectInformation\")\n","        condition = \"target.ProjectConcatenatedKey = source.ProjectConcatenatedKey\"\n","        project_delta_df.alias(\"target\").merge(project_df.alias(\"source\"), condition)\\\n","            .whenMatchedUpdate(set={\"ProjectConcatenatedKey\": \"source.ProjectConcatenatedKey\"})\\\n","            .whenNotMatchedInsert(values={\n","                \"ProjectConcatenatedKey\": \"source.ProjectConcatenatedKey\",\n","                \"ProjectName\": \"source.ProjectName\",\n","                \"RepositoryName\": \"source.RepositoryName\",\n","                \"ArtifactName\": \"source.ArtifactName\",\n","                \"ArtifactType\": \"source.ArtifactType\",\n","                \"BranchName\": \"source.BranchName\",\n","                \"ProjectInformationID\": \"source.ProjectInformationID\"\n","            }).execute()\n","    else:\n","        project_df.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/ProjectInformation\")\n"]},{"cell_type":"markdown","id":"d7740b8a-689c-4c95-90f7-41ee15225f18","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Upsert Test Results"]},{"cell_type":"code","execution_count":27,"id":"6106716a-6eb6-4879-8740-eec23de97f8e","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:53.7131655Z","execution_start_time":"2024-06-25T13:59:48.870235Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"a245d7f1-9eac-4870-939d-d057207cea23","queued_time":"2024-06-25T13:59:35.1220602Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":29,"statement_ids":[29]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 29, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["if len(target_file_names) > 0:\n","    updated_project_df = spark.sql(\"SELECT DISTINCT ProjectInformationID, ProjectConcatenatedKey FROM ProjectInformation\")\n","\n","    # Schema for Test Results\n","    schema_test_results = StructType([\n","        StructField(\"ProjectConcatenatedKey\", StringType(), True),\n","        StructField(\"ConcatenatedKey\", StringType(), True),\n","        StructField(\"RunID\", StringType(), True),\n","        StructField(\"Message\", StringType(), True),\n","        StructField(\"HasPassed\", IntegerType(), True),\n","        StructField(\"RunDate\", DateType(), True),\n","        StructField(\"RunTime\", TimestampType(), True),\n","        StructField(\"ProjectInformationID\", IntegerType(), True),\n","        # Note in a star schema this column may be better served as a dimension \n","        # but depends on what additional columns may be helpful about the tester\n","        StructField(\"Tester\", StringType(), True),        \n","        StructField(\"Order\", IntegerType(), True)\n","    ])\n","\n","    \n","    # Filter to just test results\n","    prelim_test_results_df = df.filter(df[\"IsTestResult\"] == 'True')\n","    # Update columns\n","    prelim_test_results_df = prelim_test_results_df.withColumn(\"HasPassed\",when(df[\"LogType\"].isin(\"Error\",\"Failure\"),0).otherwise(1))\n","    prelim_test_results_df = prelim_test_results_df.withColumnRenamed(\"UserName\",\"Tester\")\n","    # Left join to get project id\n","    prelim_test_results_df = prelim_test_results_df.join(updated_project_df, \\\n","                                           prelim_test_results_df.ProjectConcatenatedKey == updated_project_df.ProjectConcatenatedKey,\\\n","                                           how='left').drop(updated_project_df.ProjectConcatenatedKey)\n","    # Get specific columns\n","    prelim_test_results_df = prelim_test_results_df.select(\"ProjectConcatenatedKey\",\\\n","                                    \"ConcatenatedKey\",\\\n","                                    \"RunID\",\\\n","                                    \"Message\",\\\n","                                    \"HasPassed\",\\\n","                                    \"RunDate\",\\\n","                                    \"RunTime\",\\\n","                                    \"ProjectInformationID\",\\\n","                                    \"Tester\",\\\n","                                    \"Order\").withColumn(\"Order\", col(\"Order\").cast(\"int\"))\n","    # Enforce the schema\n","    test_results_df = spark.createDataFrame(prelim_test_results_df.toPandas(),schema_test_results)\n","    # Create TestResults Table with this schema\n","    if(not (\"TestResults\" in tables)):\n","        # Save Table\n","        test_results_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\")\\\n","                                             .partitionBy(\"RunDate\", \"ProjectInformationID\")\\\n","                                             .save(\"Tables/TestResults\")\n","    else:\n","        # Upsert\n","        testresults_delta_df = DeltaTable.forPath(spark,\"Tables/TestResults\")\n","        \n","        condition = 'target.ConcatenatedKey = source.ConcatenatedKey'\n","        testresults_delta_df.alias('target').merge(test_results_df.alias('source'), condition)\\\n","                            .whenNotMatchedInsert(values={\"ProjectConcatenatedKey\":\"source.ProjectConcatenatedKey\",\\\n","                                                        \"ConcatenatedKey\":\"source.ConcatenatedKey\",\\\n","                                                        \"RunID\":\"source.RunID\",\\\n","                                                        \"Message\":\"source.Message\",\\\n","                                                        \"HasPassed\": \"source.HasPassed\",\\\n","                                                        \"RunDate\":\"source.RunDate\",\\\n","                                                        \"RunTime\":\"source.RunTime\",\\\n","                                                        \"ProjectInformationID\":\"source.ProjectInformationID\",\\\n","                                                        \"Tester\":\"source.Tester\",\\\n","                                                        \"Order\":\"source.Order\"}).execute()\n","\n"]},{"cell_type":"markdown","id":"88af79b8-813e-4cb9-bbbf-d0fb6212d6e4","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Move files to Processed folder"]},{"cell_type":"code","execution_count":28,"id":"aa388ad7-5ba4-4efb-920d-e7c97381bf72","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:54.7505917Z","execution_start_time":"2024-06-25T13:59:54.2065793Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"eb812ad4-6444-47c1-b791-2cafcb757760","queued_time":"2024-06-25T13:59:35.2291393Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":30,"statement_ids":[30]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 30, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["# Move files to processed\n","for i, item in enumerate(target_file_names):\n","    if item.endswith('.csv'):\n","        # Get file name\n","        file = item.split(processing_path)[1]\n","        # Move Files\n","        mssparkutils.fs.mv(abfss_path + \"/\" + processing_path + file, abfss_path + \"/\" + processed_path + \"/\" + file)\n","        continue\n"]},{"cell_type":"markdown","id":"d0c7abb9-289b-4ef2-afc1-b93be9d03815","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Setup Date Dimension"]},{"cell_type":"code","execution_count":29,"id":"dddb55cb-884e-498e-abeb-7c7bd91f1145","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:58.742718Z","execution_start_time":"2024-06-25T13:59:55.195167Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"cb6d7d3e-df01-4ecf-a5ad-9e731c54275c","queued_time":"2024-06-25T13:59:35.3169962Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":31,"statement_ids":[31]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 31, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["if len(target_file_names) > 0:\n","    # Load data from TestResults table to find min and max dates\n","    testresults_stored_df = spark.sql(\"SELECT MAX(RunDate) as max, MIN(RunDate) as min FROM TestResults\")\n","\n","    # Extract start and end dates from test results\n","    start_date = testresults_stored_df.first()['min']\n","    end_date = testresults_stored_df.first()['max']\n","\n","    # List to store dates\n","    dates = []\n","\n","    # Delta to increment date in each iteration\n","    delta = timedelta(days=1)\n","    loop_date = start_date\n","\n","    # Define schema for dates DataFrame\n","    schema = StructType([\n","        StructField(\"Date\", DateType(), True),\n","        StructField(\"DayofMonth\", IntegerType(), True),\n","        StructField(\"DayName\", StringType(), True),\n","        StructField(\"Month\", IntegerType(), True),\n","        StructField(\"MonthName\", StringType(), True),\n","        StructField(\"Quarter\", IntegerType(), True),\n","        StructField(\"Year\", IntegerType(), True),\n","    ])\n","\n","    # Loop through dates from start_date to end_date\n","    while loop_date <= end_date:\n","        # Create dictionary with date attributes\n","        row = {\\\n","            \"Date\": loop_date,\\\n","            \"DayofMonth\": loop_date.day,\\\n","            \"DayName\": loop_date.strftime('%A'),\\\n","            \"Month\": loop_date.month,\\\n","            \"MonthName\": loop_date.strftime('%B'),\\\n","            \"Quarter\": ceil(loop_date.month / 3),\\\n","            \"Year\": loop_date.year\\\n","        }\n","        dates.append(row)\n","        # Increment loop_date by delta (1 day)\n","        loop_date += delta\n","\n","    # Create DataFrame from list of dates with enforced schema\n","    dates_df = spark.createDataFrame(data=dates, schema=schema)\n","\n","    # Write DataFrame to Delta format, overwriting existing data in \"Tables/Calendar\"\n","    dates_df.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/Calendar\")\n","\n"]},{"cell_type":"markdown","id":"dd223461-3b3e-4bb7-8ba7-30986be80898","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["## Setup Time Dimension"]},{"cell_type":"code","execution_count":30,"id":"18122aff-585a-4b8a-8072-587a22111bb2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-06-25T13:59:59.4741902Z","execution_start_time":"2024-06-25T13:59:59.2260911Z","livy_statement_state":"available","normalized_state":"finished","parent_msg_id":"6fdaae9f-2bab-4e84-a559-8d4ae6c26ef6","queued_time":"2024-06-25T13:59:35.4043402Z","session_id":"06114e8d-f378-47a7-b2c1-bdd22ea1b5f7","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":32,"statement_ids":[32]},"text/plain":["StatementMeta(, 06114e8d-f378-47a7-b2c1-bdd22ea1b5f7, 32, Finished, Available, Finished)"]},"metadata":{},"output_type":"display_data"}],"source":["if(not (\"Time\" in tables)):\n","    # Step 1: Define Time Dimension Schema with Hour and Minute Bins\n","    schema_time = StructType([\n","        StructField(\"Time\", TimestampType(), True),\n","        StructField(\"Hour\", IntegerType(), True),\n","        StructField(\"Minute\", IntegerType(), True),\n","        StructField(\"Second\", IntegerType(), True),\n","        StructField(\"AMPM\", StringType(), True),\n","        StructField(\"HourBin12\", IntegerType(), True),\n","        StructField(\"HourBin8\", IntegerType(), True),\n","        StructField(\"HourBin6\", IntegerType(), True),\n","        StructField(\"HourBin4\", IntegerType(), True),\n","        StructField(\"HourBin3\", IntegerType(), True),\n","        StructField(\"HourBin2\", IntegerType(), True),\n","        StructField(\"MinuteBin30\", IntegerType(), True),\n","        StructField(\"MinuteBin15\", IntegerType(), True),\n","        StructField(\"MinuteBin10\", IntegerType(), True),\n","    ])\n","\n","    # Convert 'id' column to timestamp\n","    time_df = spark.range(0, 24 * 60 * 60).selectExpr(\"timestamp(id) as Time\")\n","\n","    # Extract hour, minute, and second from the timestamp\n","    time_df = time_df.withColumn(\"Hour\", hour(\"Time\"))\n","    time_df = time_df.withColumn(\"Minute\", minute(\"Time\"))\n","    time_df = time_df.withColumn(\"Second\", second(\"Time\"))\n","    time_df = time_df.withColumn(\"AMPM\", lit(\"AM\").alias(\"AMPM\"))\n","\n","    # Calculate Hour Bins\n","    time_df = time_df.withColumn(\"HourBin12\", expr(\"Hour % 12\"))\n","    time_df = time_df.withColumn(\"HourBin8\", expr(\"Hour % 8\"))\n","    time_df = time_df.withColumn(\"HourBin6\", expr(\"Hour % 6\"))\n","    time_df = time_df.withColumn(\"HourBin4\", expr(\"Hour % 4\"))\n","    time_df = time_df.withColumn(\"HourBin3\", expr(\"Hour % 3\"))\n","    time_df = time_df.withColumn(\"HourBin2\", expr(\"Hour % 2\"))\n","\n","    # Calculate Minute Bins\n","    time_df = time_df.withColumn(\"MinuteBin30\", expr(\"Minute % 30\"))\n","    time_df = time_df.withColumn(\"MinuteBin15\", expr(\"Minute % 15\"))\n","    time_df = time_df.withColumn(\"MinuteBin10\", expr(\"Minute % 10\"))\n","\n","    # Select Columns as per Schema\n","    prelim_time_df = time_df.select(\"Time\", \"Hour\", \"Minute\", \"Second\", \"AMPM\",\n","                                    \"HourBin12\", \"HourBin8\", \"HourBin6\", \"HourBin4\", \"HourBin3\", \"HourBin2\",\n","                                    \"MinuteBin30\", \"MinuteBin15\", \"MinuteBin10\")\n","\n","    # Enforce the schema\n","    time_df = spark.createDataFrame(prelim_time_df.toPandas(),schema_time)  \n","    \n","    # Save\n","    time_df.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/Time\")"]}],"metadata":{"dependencies":{"lakehouse":{}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{"94d0bed7-1df3-4976-97ba-100160c618eb":{"persist_state":{"view":{"chartOptions":{"aggregationType":"count","binsNumber":10,"categoryFieldKeys":["0"],"chartType":"bar","isStacked":false,"seriesFieldKeys":["0"],"wordFrequency":"-1"},"tableOptions":{},"type":"details"}},"sync_state":{"isSummary":false,"is_jupyter":false,"language":"scala","table":{"rows":[{"0":"/synfs/notebook/06114e8d-f378-47a7-b2c1-bdd22ea1b5f7/lakehouse/default","1":"/lakehouse/default","2":"job","3":"abfss://c3dbac5b-48be-41eb-83a7-6c1789407037@onelake.dfs.fabric.microsoft.com/a754c80f-5f13-40b1-ab93-e4368da923c4","4":"Lakehouse","index":1},{"0":"/lakehouse/default","1":"/default","2":"default_lh","3":"abfss://c3dbac5b-48be-41eb-83a7-6c1789407037@onelake.dfs.fabric.microsoft.com/a754c80f-5f13-40b1-ab93-e4368da923c4","4":"Lakehouse","index":2},{"0":"/synfs/nb_resource/builtin","1":"/nb_resource/builtin","2":"nb_resource","3":"Notebook Working Directory","4":"Notebook Working Directory","index":3}],"schema":[{"key":"0","name":"localPath","type":"string"},{"key":"1","name":"mountPoint","type":"string"},{"key":"2","name":"scope","type":"string"},{"key":"3","name":"source","type":"string"},{"key":"4","name":"storageType","type":"string"}],"truncated":false}},"type":"Synapse.DataFrame"},"a388ce29-fdd7-4b9b-bad2-12986ddf75c6":{"persist_state":{"view":{"chartOptions":{"aggregationType":"count","binsNumber":10,"categoryFieldKeys":["0"],"chartType":"bar","isStacked":false,"seriesFieldKeys":["0"],"wordFrequency":"-1"},"tableOptions":{},"type":"details"}},"sync_state":{"isSummary":false,"is_jupyter":false,"language":"scala","table":{"rows":[{"0":"/synfs/notebook/06114e8d-f378-47a7-b2c1-bdd22ea1b5f7/lakehouse/default","1":"/lakehouse/default","2":"job","3":"abfss://c3dbac5b-48be-41eb-83a7-6c1789407037@onelake.dfs.fabric.microsoft.com/a754c80f-5f13-40b1-ab93-e4368da923c4","4":"Lakehouse","index":1}],"schema":[{"key":"0","name":"localPath","type":"string"},{"key":"1","name":"mountPoint","type":"string"},{"key":"2","name":"scope","type":"string"},{"key":"3","name":"source","type":"string"},{"key":"4","name":"storageType","type":"string"}],"truncated":false}},"type":"Synapse.DataFrame"}},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
